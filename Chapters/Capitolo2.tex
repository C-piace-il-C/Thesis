% Chapter Template

\chapter{Calcolatori e dati} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Capitolo 2. \emph{Calcolatori e dati}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

Negli ultimi due secoli il concetto di ``calcolatore'', a cui è subentrato nel
lessico comune il termine \emph{computer}, si è ampiamente esteso. Nei primi
anni del XIX secolo vennero poste le basi concettuali del computer programmabile, 
il primo modernamente definibile Turing-completo, da parte di Charles Babbage 
(non a caso considerato il padre del computer\citep{CBabbage})
e negli anni '30 del XX secolo proprio Alan Turing definì i principi dell' 
odierno computer. 
\\
Il calcolatore, dunque, passa dall'essere uno strumento usato per
eseguire semplici calcoli matematici (in questa categoria potrebbe rientrare
anche un abaco) a macchina capace di eseguire calcoli matematici anche molto 
complessi (a cui ci si riferisce talvolta con il termine \emph{elaboratore}).
%-------------------------------------------------------------------------------
%	SECTION 1
%-------------------------------------------------------------------------------

\section{Acquisizione ed elaborazione dei dati}

Affinché un computer possa eseguire dei calcoli è necessario che disponga di
\emph{dati} su cui effettuarli. L'acquisizione di questi ultimi può avvenire
in diversi modi, con o senza l'intervento di un essere umano.
Nel secondo caso i dati vengono spesso ottenuti dalla trasduzione 
di parametri fisici, acquisiti da sensori, in segnali elettrici, successivamente
tradotti da un convertitore analogico-digitale in modo da ottenere valori che 
possano essere compresi da un calcolatore digitale.
\\ \\
In entrambi i casi è raro che un'acquisizione sia fine a sé stessa e venga
semplicemente memorizzata dentro ad un supporto digitale: che sia ricavata una
banale statistica o vengano effettuati trasformazioni e calcoli più complessi,
è necessario che i dati vengano elaborati. \\
Un'elaborazione può preservare la struttura originaria di un dato (come avviene
quando si utilizza quest'ultimo come risorsa per effettuare un calcolo) o
modificarla in modo reversibile o irreversibile (rispettivamente in caso, ad
esempio, di una compressione senza o con perdita). 
\\ \\
Qualunque sia il tipo di elaborazione che deve essere svolta è quasi sempre
indispensabile poterla effettuare entro certi limiti temporali, sia per
volontà personale di ottenere dei risultati in tempo breve, sia per
necessità imposta dal lavoro che si sta svolgendo (riportando l'esempio fornito
nell'introduzione, in caso si debba effettuare una codifica video in tempo reale
risulta inaccettabile farlo a meno di 24 fotogrammi al secondo); un fattore di
grande importanza nell'elaborazione dei dati risiede dunque anche nella potenza
di calcolo di cui si dispone per svolgerla.
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Potenza di calcolo}
Il termine ``potenza di calcolo'' può essere talvolta fuorviante, in quanto
tende ad essere intesa come la capacità di un elaboratore di eseguire uno
specifico calcolo nel minor tempo possibile: questa è una delle discriminanti
nell'identificazione della potenza di calcolo di un computer, ma non l'unica. 
\\ \\
Se il proprio progetto prevedesse di scaricare da un \emph{server} una grande
mole di dati, effettuare le dovute elaborazioni e caricare il risultato 
ottenuto, risulterebbe evidente come non basti solamente effettuare i calcoli
velocemente: se questi ultimi fossero eseguiti in pochi secondi e il
caricamento/scaricamento richiedesse ore, la potenza di calcolo percepita
dal \emph{server} sarebbe nettamente inferiore di quella che si potrebbe
pensare. \\
Un simile discorso può essere intrapreso con diverse variabili che influenzano
la percezione della potenza di calcolo, tra cui le necessità di chi deve
usufruire di quest'ultima; vengono presentati di seguito i fattori più
comuni per stimare una generica potenza di calcolo:
\newpage
\begin{itemize}
\item Potenza \emph{grezza} del processore (MIPS, MFLOPS)
\item Latenza degli accessi in memoria
\item Latenza delle interfacce I/O
\item Bandwidth
\item Throughput
\end{itemize}
Ognuno di questi fattori ha subito negli anni dei miglioramenti, sopratutto il
primo grazie alla continua diminuzione della dimensione dei transistor. 
%diminuzione del processo costruttivo non ha senso
\\ \\
Gordon Moore predisse nel 1965\citep{GMoore} che, 
per almeno dieci anni, il numero di transistor in un circuito integrato 
sarebbe raddoppiato ogni anno (a parità di costo di produzione del circuito). \\
Nel 1975 stimò, per la decade successiva, un periodo di due anni come 
necessario per ottenere lo stesso incremento. Quest'ultima affermazione, 
nota come ``Legge di Moore'', sebbene abbia mostrato alcuni segni di
cedimento negli ultimi anni\citep{MooresLaw}, ha 
predetto correttamente l'andamento dello sviluppo tecnologico degli ultimi
cinquant'anni; una sempre maggiore \emph{densità} di transistor a parità
di costo ha portato ad un conseguente aumento della capacità di calcolo 
disponibile (più transistor significano più unità logiche e matematiche,
più memoria, più interfacce I/O, etc.), rendendo possibili elaborazioni
sempre più complesse. 
\\ \\
Come intuibile dal precedente paragrafo, i limiti fisici hanno imposto ad un 
certo punto un cambio di strategia nella progettazione dei circuiti integrati,
soprattutto le \emph{central processing unit} (CPU).
\\ \\
A metà del 2005 si era giunti ad un punto in cui non era più possibile 
aumentare la potenza di calcolo di un processore semplicemente introducendo
più transistor e incrementando la frequenza di lavoro: sebbene Intel avesse 
dichiarato possibile raggiungere frequenze pari a 10GHz, dovette presto 
ricredersi quando, raggiunti i 3.8GHz con il Pentium 4 670, si verificarono 
fenomeni di dispersione di potenza nei transistor ad un ulteriore aumento della 
frequenza, con un processo produttivo di 90nm. \\
Questo, e le temperature raggiunte da così tanti transistor che operano a 
frequenze così elevate, ha reso necessario un cambio di mentalità, facendo 
passare da una ``corsa al GHz'' all'inclusione di più processori (che 
prendono il nome di \emph{core}) sullo stesso \emph{die}. 
\\ \\
Le configurazioni \emph{multi core} hanno permesso che continuasse il 
trend %questa è nel dizionario italiano
 dell'incremento della potenza in concomitanza con il crescere 
del numero dei transistor, introducendo però una grande complicazione nella 
progettazione di un software che volesse sfruttare questo parallelismo. \\
È infatti necessario prestare particolari accorgimenti nella realizzazione 
di codici parallelizzati, in quanto è molto probabile, se non si presta la 
dovuta attenzione, creare situazioni di \emph{race condition}, in cui due (o 
più) \emph{thread} devono accedere e modificare la stessa risorsa globale; 
senza una sincronizzazione o la definizione di una operazione mutuamente 
esclusiva, il comportamento dei due \emph{thread} non è prevedibile a priori. \\
Ad esempio, se entrambi cercassero di accedere alla stessa variabile per 
incrementarla, a seconda dell'ordine in cui verranno eseguite queste operazioni 
cambierà il risultato ottenuto:
\begin{table}[H]
  \centering
  \subfloat[Comportamento corretto \label{tab:corretto}]
  {
    \centering
    \begin{tabular}{|c|c|c|c|}
      \hline
      
      \multicolumn{1}{|c }{\textbf{Thread 1}} &
      \multicolumn{1}{|c|}{\textbf{Thread 2}} &
      \multicolumn{1}{ c|}{                 } &
      \multicolumn{1}{ c|}{\textbf{Valore}  } \\
      
      \hline
      \hline
      \hline
      
      Read value     &                & $\leftarrow $ & 0 \\
      Increase value &                &               & 0 \\
      Write back     &                & $\rightarrow$ & 1 \\
                     & Read value     & $\leftarrow $ & 1 \\
                     & Increase value &               & 1 \\
                     & Write back     & $\rightarrow$ & 2 \\
                     
      \hline
    \end{tabular}
  }\qquad\qquad 
  \subfloat[Comportamento non corretto \label{tab:non_corretto}]
  {
    \centering
    \begin{tabular}{|c|c|c|c|}
      \hline
      
      \multicolumn{1}{|c }{\textbf{Thread 1}} &
      \multicolumn{1}{|c|}{\textbf{Thread 2}} &
      \multicolumn{1}{ c|}{                 } &
      \multicolumn{1}{ c|}{\textbf{Valore}  }	\\
      
      \hline
      \hline
      \hline
      
      Read value     &                & $\leftarrow $  & 0 \\
                     & Read value     & $\leftarrow $  & 0 \\
      Increase value &                &                & 0 \\
                     & Increase value &                & 0 \\
      Write back     &                & $\rightarrow$  & 1 \\
                     & Write back     & $\rightarrow$  & 1 \\
                     
      \hline
    \end{tabular}
  }
\end{table}

Sebbene il \emph{multithreading} sia una risorsa indubbiamente utile, è
importante prestare la dovuta attenzione nella realizzazione di \emph{software}
che desideri sfruttare questa possibilità.
%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Ottimizzazione di algoritmi}

La sola potenza di calcolo %o pura o di per sé
non è lo strumento ottimale con cui si riesce a
ridurre il tempo impiegato ad effettuare un'elaborazione, ma è possibile agire
su un altro aspetto del \emph{computing} che porta enormi benefici a prescindere
dalla potenza disponibile, ovverosia l'ottimizzazione dell'algoritmo utilizzato
per compiere l'elaborazione desiderata.
L'ottimizzazione di un algoritmo consiste tipicamente nella ricerca e 
successiva rimozione di operazioni inutili al fine di incrementarne 
l'efficienza.
 %non confondere l'ottimizzazione dell'algoritmo con l'ottimizzazione del 
 %codice: ottimizzare un algoritmo significa diminuire il numero di operazioni 
 %per svolgere un lavoro, che non significa proprio sfruttare più risorse 
 %disponibili nella piattaforma.
\\ \\ 
Introducendo un esempio che è caratteristico dell'ambito elettronico e
telematico, si pensi all'algoritmo per il calcolo della trasformata discreta
di Fourier; traducendo ``letteralmente'' la formula matematica in codice
compilabile ed eseguibile da un calcolatore, si ottiene che il peso
computazionale risulta $O(n^2)$ (ovvero, per una sequenza di $N$ 
elementi servono $N^2$ operazioni), quindi la debolezza di questo algoritmo 
%continui a confondere algoritmo e implementazione
si percepisce di più al crescere della sequenza: %la debolezza non aumenta: 
%sono i suoi effetti che aumentano
 se quest'ultima fosse lunga 4'096 elementi
sarebbero necessarie quasi diciassette milioni di somme e moltiplicazioni. \\
Se invece si sfruttano le proprietà di simmetria della DFT calcolando 
quest'ultima attraverso una suddivisione ricorsiva su DFT più piccole 
(\emph{decimazione nel tempo}) si ottiene un peso computazionale pari a 
$N\log_{2}N$, che scende a $\frac{N}{2}\log_{2}\frac{N}{2}$ se la sequenza è 
reale, %Se la sequenza è reale devo calcolarla su N/2 punti..
come nella grande maggioranza dei casi. %davvero troppi incisi
\\ Riutilizzando l'esempio precedente, si nota che in presenza di una sequenza 
di 4096 elementi diventano necessarie solo circa 50'000 somme e 
moltiplicazioni. %nell'esempio precedente non si parla di sequenze reali, 
%comunque sarebbe 4096/2 log2(4096/2) = 22528, quindi circa 22'500.. non 
%sarebbe comunque bello parlare solo qui di sequenze reali in quanto sembra 
%un'ottimizzazione dovuta alla trasformata veloce. Ho tolto il "tra somme e 
%moltiplicazioni" perché per essere precisi, la FFT richiede N/2log2(N) 
%Moltiplicazioni + Nlog2(N) somme
\\ \\
Il fattore di \emph{speed-up} del precedente esempio risulta pari a $340\times$
 pur volendo essere ottimisti ed 
ipotizzando che la prima previsione di Moore fosse rimasta tale all'infinito, 
sarebbero comunque serviti rispettivamente circa 8.5 e 9.5 anni di sviluppo 
tecnologico per ottenere un miglioramento paragonabile affidandosi alla pura 
potenza bruta di un processore, ma avendo in ogni caso un algoritmo che possiede
grandi limitazioni con sequenze di dimensione crescente. %molto bello questo 
%discorso