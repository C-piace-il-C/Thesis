% Chapter Template

\chapter{Calcolatori e dati} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Capitolo 2. \emph{Calcolatori e dati}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

Negli ultimi due secoli il concetto di ``calcolatore'', a cui è subentrato nel
lessico comune il termine \emph{computer}, si è ampiamente esteso. Nei primi
anni del XIX secolo vennero poste le basi concettuali del computer programmabile, 
il primo modernamente definibile Turing-completo, da parte di Charles Babbage 
(non a caso considerato il padre del computer\citep{CBabbage})
e negli anni '30 del XX secolo proprio Alan Turing definì i principi del odierno computer. 
\\
Il calcolatore, dunque, passa dall'essere uno strumento usato per
eseguire semplici calcoli matematici (in questa categoria potrebbe rientrare
anche un abaco) a macchina capace di eseguire calcoli matematici anche molto 
complessi (a cui ci si riferisce talvolta con il termine \emph{elaboratore}).
\\
// Aggiungere?

%-------------------------------------------------------------------------------
%	SECTION 1
%-------------------------------------------------------------------------------

\section{Acquisizione ed elaborazione dei dati}

Affinché un computer possa eseguire dei calcoli è necessario che disponga di
\emph{dati} su cui effettuarli. L'acquisizione di questi ultimi può avvenire
in diversi modi, con o senza l'intervento di un essere umano.
Nel secondo caso i dati vengono spesso ottenuti grazie alla trasduzione 
di parametri fisici, acquisiti da sensori, in segnali elettrici, successivamente
tradotti da un convertitore analogico-digitale in modo da ottenere valori che 
possano essere compresi da un calcolatore binario.
\\ \\
In entrambi i casi è raro che un'acquisizione sia fine a sé stessa e venga
semplicemente memorizzata dentro ad un supporto digitale: che sia ricavata una
banale statistica o vengano effettuati trasformazioni e calcoli più complessi,
è necessario che i dati vengano elaborati. \\
Un'elaborazione può preservare la struttura originaria di un dato (come avviene
quando si utilizza quest'ultimo come risorsa per effettuare un calcolo) o
modificarla in modo reversibile o irreversibile (rispettivamente in caso, ad
esempio, di una compressione senza o con perdita). 
\\ \\
Qualunque sia il tipo di elaborazione che deve essere svolta è quasi sempre
indispensabile poterla effettuare entro certi limiti temporali, sia per
volontà personale di ottenere dei risultati in tempo breve, sia per
necessità imposta dal lavoro che si sta svolgendo (riportando l'esempio fornito
nell'introduzione, in caso si debba effettuare una codifica video in tempo reale
risulta inaccettabile farlo a meno di 24 fotogrammi al secondo); un fattore di
grande importanza nell'elaborazione dei dati risiede dunque anche nella potenza
di calcolo di cui si dispone per svolgerla.
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Potenza di calcolo}
Il termine ``potenza di calcolo'' può essere talvolta fuorviante, in quanto
tende ad essere intesa come la capacità di un elaboratore di eseguire uno
specifico calcolo nel minor tempo possibile: questa è una delle discriminanti
nell'identificazione della potenza di calcolo di un computer, ma non l'unica. 
\\ \\
Se il proprio progetto prevedesse di scaricare da un \emph{server} una grande
mole di dati, effettuare le dovute elaborazioni e caricare il risultato 
ottenuto, risulta evidente come non basti solamente effettuare i calcoli
velocemente: se questi ultimi fossero eseguiti in pochi secondi e il
caricamento/scaricamento richiedesse ore, la potenza di calcolo percepita
dal \emph{server} sarebbe nettamente inferiore di quella che si potrebbe
pensare. \\
Un simile discorso può essere intrapreso con diverse variabili che influenzano
la percezione della potenza di calcolo, tra cui le necessità di chi deve
usufruire di quest'ultima; vengono presentati di seguito i fattori più
comunemente sovrapposti per approssimare il calcolo di una potenza di calcolo
generica:
\newpage
\begin{itemize}
\item Potenza \emph{grezza} del processore (MIPS, MFLOPS)
\item Latenza degli accessi in memoria
\item Latenza delle interfacce I/O
\item Bandwidth
\item Throughput
\end{itemize}
Ognuno di questi fattori ha subito negli anni dei miglioramenti, sopratutto il
primo grazie alla continua diminuzione del processo produttivo usato per creare
i transistor.
\\ \\
Gordon Moore predisse nel 1965\citep{GMoore} che, 
per almeno dieci anni, il numero di transistor in un circuito integrato 
sarebbe raddoppiato ogni anno (a parità di costo di produzione del circuito). \\
Nel 1975 stimò, per la decade successiva, un periodo di due anni come 
necessario per ottenere lo stesso incremento. Quest'ultima affermazione, 
nota come ``Legge di Moore'', sebbene abbia mostrato alcuni segni di
cedimento negli ultimi anni\citep{MooresLaw}, ha 
predetto correttamente l'andamento dello sviluppo tecnologico degli ultimi
cinquant'anni; una sempre maggiore \emph{densità} di transistor a parità
di costo ha portato ad un conseguente aumento della capacità di calcolo 
disponibile (più transistor significano più unità logiche e matematiche,
più memoria, più interfacce I/O, etc.), rendendo possibili elaborazioni
sempre più complesse. 
\\ \\
Come intuibile dal precedente paragrafo, i limiti fisici hanno imposto ad un 
certo punto un cambio di strategia nella progettazione dei circuiti integrati,
soprattutto le \emph{central processing unit} (CPU).
\\ \\
A metà del 2005 si era giunti ad un punto in cui non era più possibile 
aumentare la potenza di calcolo di un processore semplicemente introducendo
più transistor e incrementando la frequenza di lavoro: sebbene Intel avesse 
dichiarato possibile raggiungere frequenze pari a 10GHz, dovette presto 
ricredersi quando, raggiunti i 3.8GHz con il Pentium 4 670, si verificarono 
fenomeni di dispersione di potenza nei transistor ad un ulteriore aumento della 
frequenza, con un processo produttivo di 90nm. \\
Questo, e le temperature raggiunte da così tanti transistor che operano a 
frequenze così elevate, ha reso necessario un cambio di mentalità, facendo 
passare da una ``corsa al GHz'' a un inclusione di più processori (che 
prendono il nome di \emph{core}) sullo stesso \emph{die}. 
\\ \\
Le configurazioni \emph{multi core} hanno permesso che continuasse il 
\emph{trend} dell'incremento della potenza in concomitanza con il crescere 
del numero dei transistor, introducendo però una grande complicazione nella 
progettazione di un software che volesse sfruttare questo parallelismo. \\
È infatti necessario prestare particolari accorgimenti nella realizzazione 
di codici parallelizzati, in quanto è molto probabile, se non si presta la 
dovuta attenzione, creare situazioni di \emph{race condition}, in cui due (o 
più) \emph{thread} devono accedere e modificare la stessa risorsa globale; 
senza una sincronizzazione o la definizione di una operazione mutuamente 
esclusiva, il comportamento dei due \emph{thread} non è prevedibile a priori. \\
Ad esempio, se entrambi cercassero di accedere alla stessa variabile per 
incrementarla, a seconda dell'ordine in cui verranno eseguite queste operazioni 
cambierà il risultato ottenuto:
\begin{table}[H]
  \centering
  \subfloat[Comportamento corretto \label{tab:corretto}]
  {
    \centering
    \begin{tabular}{|c|c|c|c|}
      \hline
      
      \multicolumn{1}{|c }{\textbf{Thread 1}} &
      \multicolumn{1}{|c|}{\textbf{Thread 2}} &
      \multicolumn{1}{ c|}{                 } &
      \multicolumn{1}{ c|}{\textbf{Valore}  } \\
      
      \hline
      \hline
      \hline
      
      Read value     &                & $\leftarrow $ & 0 \\
      Increase value &                &               & 0 \\
      Write back     &                & $\rightarrow$ & 1 \\
                     & Read value     & $\leftarrow $ & 1 \\
                     & Increase value &               & 1 \\
                     & Write back     & $\rightarrow$ & 2 \\
                     
      \hline
    \end{tabular}
  }\qquad\qquad 
  \subfloat[Comportamento non corretto \label{tab:non_corretto}]
  {
    \centering
    \begin{tabular}{|c|c|c|c|}
      \hline
      
      \multicolumn{1}{|c }{\textbf{Thread 1}} &
      \multicolumn{1}{|c|}{\textbf{Thread 2}} &
      \multicolumn{1}{ c|}{                 } &
      \multicolumn{1}{ c|}{\textbf{Valore}  }	\\
      
      \hline
      \hline
      \hline
      
      Read value     &                & $\leftarrow $  & 0 \\
                     & Read value     & $\leftarrow $  & 0 \\
      Increase value &                &                & 0 \\
                     & Increase value &                & 0 \\
      Write back     &                & $\rightarrow$  & 1 \\
                     & Write back     & $\rightarrow$  & 1 \\
                     
      \hline
    \end{tabular}
  }
\end{table}

Sebbene il \emph{multithreading} sia una risorsa indubbiamente utile, è
importante prestare la dovuta attenzione nella realizzazione di \emph{software}
che desideri sfruttare questa possibilità.
%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Ottimizzazione di algoritmi}

La pura potenza di per sé non è lo strumento ottimale con cui si riesce a
ridurre il tempo impiegato ad effettuare un'elaborazione, ma è possibile agire
su un altro aspetto del \emph{computing} che porta enormi benefici a prescindere
dalla potenza disponibile, ovverosia l'ottimizzazione dell'algoritmo utilizzato
per compiere l'elaborazione desiderata. \\
Un algoritmo non ottimizzato, che non sfrutta quindi tutte le potenzialità
offerte dalla piattaforma su cui viene implementato, risulterà sempre più lento 
e meno efficiente di un algoritmo scritto in modo tale da usufruire di tutte le
caratteristiche del sistema in cui viene implementato; ciò è valido quasi 
sempre, eccezion fatta per quando la seconda piattaforma è estremamente meno
performante della prima\footnote{In questo caso la seconda piattaforma risulta
più lenta, ma rimane comunque più efficiente.}.
\\ \\ 
Introducendo un esempio che è caratteristico dell'ambito elettronico e
telematico, si pensi all'algoritmo per il calcolo della trasformata discreta
di Fourier; traducendo ``letteralmente'' la formula matematica in codice
compilabile ed eseguibile da un calcolatore, si ottiene che il peso
computazionale risulta $O(n^2)$ (ovvero, per una sequenza di $N$ elementi
servono $N^2$ operazioni), quindi la debolezza di questa implementazione aumenta
al crescere della sequenza: se quest'ultima fosse lunga 4'096 elementi
sarebbero necessarie quasi diciassette milioni di somme e moltiplicazioni. \\
Se invece si sfruttano le proprietà della DFT e si calcola quest'ultima
come un calcolo successivo di DFT più piccole (suddividendo la sequenza $x(n)$
in ingresso in sottosequenze sempre più piccole, effettuando dunque una
\emph{decimazione nel tempo}) si ottiene un peso computazionale pari a 
$Nlog_{2}(N)$, che scende a $\frac{N}{2}log_{2}(N)$ se la sequenza è reale
(ovverosia nella grande maggioranza dei casi). \\
Riutilizzando l'esempio precedente, si nota come in presenza di una sequenza di 
4096 elementi sarebbero necessarie rispettivamente circa 50'000 e 25'000 tra
somme e moltiplicazioni. 
\\ \\
Il fattore di \emph{speed-up} del precedente esempio risulta pari a 340$\times$
nel primo caso e 680$\times$ nel secondo: pur volendo essere ottimisti ed 
ipotizzando che la prima previsione di Moore fosse rimasta tale all'infinito, 
sarebbero comunque serviti rispettivamente circa 8.5 e 9.5 anni di sviluppo 
tecnologico per ottenere un miglioramento paragonabile affidandosi alla pura 
potenza bruta di un processore, ma avendo in ogni caso un algoritmo che possiede
grandi limitazioni con sequenze di dimensione crescente.