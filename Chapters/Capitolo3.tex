% Chapter Template

\chapter{La necessità di comprimere i dati} % Main chapter title

\label{Chapter3}

\lhead{Capitolo 3. \emph{La necessità di comprimere i dati}}

%-------------------------------------------------------------------------------
%	SECTION 1
%-------------------------------------------------------------------------------

\section{Compressione dati}
La disponibilità di calcolatori sempre più potenti ad un costo sempre più
contenuto ha portato alla conseguente possibilità di effettuare elaborazioni
via via più complesse, incrementando di pari passo la mole di dati su cui
queste elaborazioni vengono svolte. \\
Per fare un esempio nel 1988 venne proposto, all'interno di H.261, lo standard
\emph{Common Intermediate Format} (CIF) per uniformare la risoluzione verticale
ed orizzontale delle sequenze video solitamente utilizzate nelle teleconferenze.
Lo standard prevede una risoluzione pari a 352${\times}$288 pixel ed un
\emph{frame rate} pari a 30000/1001 (circa 29.97) \emph{frame} al secondo;
prendendo in considerazione una codifica senza compressione in uno spazio
colore RGB con 8 bit per canale ed applicando la formula
\begin{align*}
  \text{MB}\!/\!\text{s} = 
  W \cdot H \cdot (bit\_depth \:/\: 3) \cdot frames\!/\!\text{s} \:/\: 1000000
\end{align*}
si ottiene che un secondo di sequenza video pesa circa 9 \emph{megabyte}. \\
Considerando la capacità tipica di un disco rigido dell'epoca (compresa
tra i 20 e i 60MB) o la larghezza di banda 
necessaria a trasmettere una tale sequenza, risulta evidente come la 
compressione dei dati sia stata una grande necessità; se effettuiamo lo stesso 
calcolo utilizzando una risoluzione standard moderna (e.g., 1920$\times$1080), 
otteniamo un peso di circa 187MB per ogni secondo contenuto nella sequenza 
video. \\
Sebbene a tutt'oggi le capacità di memorizzazione siano incrementate di fattori
che variano tra le 12'500 e le 200'000 volte\footnote{Sono state confrontate
rispettivamente capacità di 500GB e 8TB con la capacità media di 40MB 
dell'epoca.}, la velocità di trasmissione dei dati non è aumentata di pari 
passo; la compressione dei dati rimane comunque anche un'ottima risorsa per
poter immagazzinare grandi quantità di dati con la possibilità di preservare
la qualità originale di questi ultimi.
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Compressione \emph{lossless}}

Gli algoritmi di compressione senza perdita (\emph{lossless}) sfruttano
solitamente le ridondanze, di qualsiasi tipo esse siano,  per rappresentare i
dati nel modo più sintetico possibile senza andare ad intaccare il messaggio
originale, ovvero senza causare alcuna perdita d'informazione.\\

Il limite di questa classe di algoritmi è definito dal primo teorema di 
Shannon, che definisce il significato operativo di entropia e vincola la 
massima compressione ottenibile; eccetto alcuni casi particolari è estremamente 
dispendioso in termini computazionali avvicinarsi all'esatto limite teorico 
della compressione.\\

Queste tecniche sono necessarie laddove non sia ammessa la corruzione del dato 
originale: compressione di documenti e programmi ma anche di audio e video ad 
alta qualità, per applicazioni professionali o di estrazione d'informazione da 
parte di un calcolatore.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Compressione \emph{lossy}}

Se l'applicazione non richiede una ricostruzione esatta del messaggio originale 
è possibile utilizzare più efficienti algoritmi di compressione con perdita 
(\emph{lossy}).
Questa classe di algoritmi ha solitamente come fruitore del risultato l'uomo:
per questo motivo vengono sfruttate le conoscenze che si hanno dell'apparato 
audio-visivo al fine di rendere impercettibile la perdita d'informazione, in 
aggiunta a tutte le tecniche di compressione \emph{lossless}.
La resa in termini di dimensioni del dato compresso è molto superiore rispetto 
alla variante senza perdita: nel caso di audio ed immagini la dimensione del 
risultato in uscita rispetto all'originale passa in media dal $50\%$ della 
variante \emph{lossless} al $10\%$ se viene utilizzata una compressione 
\emph{lossy}.

%-------------------------------------------------------------------------------
%	SECTION 2
%-------------------------------------------------------------------------------

\section{Compressione di immagini}

% \subsection{Il sistema visivo umano}

%-----------------------------------
%       SUBSECTION 1
%-----------------------------------

\subsection{Utilizzo delle trasformate}

I metodi più diffusi per quanto riguarda la compressione di immagini prevedono 
l'utilizzo di trasformate con lo scopo di rendere l'immagine più adatta ad 
essere codificata: questa viene prima divisa in blocchi di dimensioni ridotte, 
$N \times N$, solitamente quadrate e non superiori a $64 \times 64$.\\

Su ogni blocco viene effettuata separatamente una trasformata, 
il cui obiettivo è quello di decorrelare il più possibile il segnale originale.
La bontà di una trasformata viene solitamente definita in base alle sue 
capacità di decorrelazione e implementazione veloce.\\

Tra le trasformate si ricordano:

\begin{itemize}
  
  \item \textbf{\emph{Discrete Fourier transform} (DFT)}\\
    Largamente utilizzata in analisi e filtraggio, ha la proprietà di avere un 
    nucleo separabile (rendendo possibile quindi il calcolo della trasformata 
    isolatamente su righe e colonne). Ha una sua implementazione veloce, 
    \emph{fast Fourier transform} (FFT), che rende il suo utilizzo molto 
    desiderabile.
    
  \item \textbf{\emph{Karhunen–Loève transform} (KLT)}\\
    Fornisce la miglior compressione d'energia rispetto alle altre 
    trasformazioni possibili. Purtroppo la mancanza di un algoritmo veloce e la 
    necessità di uno studio della covarianza dell'immagine da comprimere per 
    generare le funzioni base la rendono scarsamente utilizzata.
    
  \item \textbf{\emph{Discrete Cosine transform} (DCT)}\\
    Molto simile alla DFT, da cui differisce per l'utilizzo di soli coseni come 
    funzioni base.
    Permette di ottenere una decorrelazione molto simile a quella ottenibile 
    trasformando con la KLT. La presenza di un algoritmo veloce la rende 
    tutt'ora la trasformata più utilizzata per la compressione d'immagini.
   
  \item \textbf{\emph{Walsh-Hadamard transform} (WHT)}\\
    La peggiore in termini di compattamento d'energia, ha la proprietà di poter 
    essere eseguita con sole somme e sottrazioni ed ha una sua versione 
    \emph{fast}; il bassissimo costo computazionale l'ha resa largamente 
    utilizzata.
    
\end{itemize}

%-----------------------------------
%       SUBSECTION 2
%-----------------------------------

\subsection{Principali algoritmi}

\paragraph{\textit{Run-length encoding} \\}
  La compressione RLE è una forma molto semplice di codifica d'immagine 
  \emph{lossless}: applicata solitamente a immagini in bianco e nero, sfrutta 
  il fatto che pixel bianchi e neri si presentano in sequenza. Invece di 
  trasmettere singolarmente i bit dell'immagine vengono trasmesse le lunghezze 
  (\emph{length}) dei tratti (\emph{run}) aventi lo stesso colore.

\paragraph{\textit{Differential Pulse-Code Modulation lossless} \\}
  La codifica DPCM sfrutta la correlazione tra i \emph{pixel} adiacenti di 
  un'immagine: si prova a predire il valore di un pixel a partire da quelli 
  vicini già conosciuti. Questo tipo di codifica permette di avvicinarsi molto 
  al limite massimo di compressione \emph{lossless}.\\
  Il principio di funzionamento è molto semplice: si genera l'immagine predetta 
  e se ne fa la differenza con l'originale, ricavando quindi la funzione 
  ``errore'', che verrà in seguito codificata. Il vantaggio rispetto alla 
  semplice codifica dei pixel risiede nel fatto che la suddetta funzione è molto
  meno correlata all'immagine rispetto all'originale.

\paragraph{\textit{Joint Photographic Experts Group} \\}
  JPEG è un formato molto utilizzato per la compressione \emph{lossy} di 
  immagini digitali, in particolar modo per quelle fotografiche. La codifica 
  JPEG sfrutta alcune peculiarità del sistema visivo umano per poter comprimere 
  il più possibile un'immagine senza che il fruitore (l'uomo) possa accorgersi 
  della differenza rispetto all'orginale. Non esiste una sola versione 
  dell'algoritmo: JPEG/Exif è il più comune nelle immagini generate da 
  fotocamere digitali mentre JPEG/JFIF è utilizzato maggiormente per salvare e 
  trasmettere immagini su Internet, questo per citare solo le due versioni più 
  utilizzate.

%TODO chroma subsampling

%-------------------------------------------------------------------------------
%       SECTION 3
%-------------------------------------------------------------------------------

\section{Compressione video}

La compressione di dati assume un ruolo fondamentale nella trasmissione o 
memorizzazione di sequenze video: citando l'introduzione è assolutamente 
impensabile non comprimere uno \emph{stream} a 1080p, che occupa circa 187MB 
per secondo di video. In seguito verranno descritti i principi base che 
governano tutti gli algoritmi di compressione d'immagini in sequenza.

%-----------------------------------
%       SUBSECTION 1
%-----------------------------------

\subsection{Ridondanza spaziale}

Sfrutta il fatto che ogni immagine (\emph{frame}) del video, presa 
singolarmente, può essere compressa: valgono infatti tutte le teorie della 
compressione di immagini trattate nel capitolo precedente.
La sola ridondanza spaziale non è però sufficiente a garantire una buona 
dimensione del video compresso, a meno ovviamente di non rinunciare a parecchia 
qualità.

%-----------------------------------
%       SUBSECTION 2
%-----------------------------------

\subsection{Ridondanza temporale}

La vera differenza tra un algoritmo di compressione d'immagine ed uno video sta 
nello sfruttare la ridondanza temporale dei \emph{frame}. Essendo il video 
infatti nient'altro che una serie di fotografie prese ad intervalli costanti e 
molto ravvicinati (il minimo numero possibile affinché l'occhio abbia la 
percezione di movimento è $24$ immagini al secondo), si sfrutta il fatto che 
tra due immagini ``vicine'' vi sia una grande correlazione temporale. 
Infatti solitamente, a meno di situazioni particolari (come un cambio di scena 
o un movimento brusco), in due \emph{frame} adiacenti vi sono gli stessi oggetti
al più spostati di poco rispetto alla loro posizione originale.\\

Questa intuizione permette di andare molto oltre il classico $10\%$ della mera 
compressione basata su ridondanza spaziale arrivando ad addirittura allo 
$0.39\%$\footnote{Utilizzando un video di risoluzione 4k ($3996\times2160$ 
pixel), H.265 genera un \emph{bit-rate} di circa $3$ MB/s contro i circa $777$ 
della versione non compressa.} del file video originale. 

%-----------------------------------
%       SUBSECTION 3
%-----------------------------------

%\subsection{Principali algoritmi}