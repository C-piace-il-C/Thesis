% Chapter Template

\chapter{La necessità di comprimere i dati} % Main chapter title

\label{Chapter3}

\lhead{Capitolo 3. \emph{La necessità di comprimere i dati}}

%-------------------------------------------------------------------------------
%	SECTION 1
%-------------------------------------------------------------------------------

\section{Compressione dati}
La disponibilità di calcolatori sempre più potenti ad un costo sempre più
contenuto ha portato alla conseguente possibilità di effettuare elaborazioni
via via più complesse, incrementando di pari passo la mole di dati su cui
queste elaborazioni vengono svolte. \\
(Per fare un esempio) nel 1988 venne proposto, all'interno di H.261, lo standard
\emph{Common Intermediate Format} (CIF) per uniformare la risoluzione verticale
ed orizzontale delle sequenze video solitamente utilizzate nelle teleconferenze.
Lo standard prevede una risoluzione pari a 352${\times}$288 pixel ed un
\emph{frame rate} pari a 30000/1001 (circa 29.97) \emph{frames} al secondo;
prendendo in considerazione una codifica senza compressione in uno spazio
colore RGB con 8 bit per canale ed applicando la formula
\begin{align*}
  \text{MB}\!/\!\text{s} = 
  W \times H \times bit\_depth \times frames\!/\!\text{s} \:/\: 1000
\end{align*}
si ottiene che un secondo di sequenza video pesa circa 9 \emph{megabyte}. \\
Considerando la capacità tipica di un disco rigido dell'epoca (compresa
tra i 20 e i 60MB$^{[citazione\: necessaria]}$) o la larghezza di banda 
necessaria a trasmettere una tale sequenza, risulta evidente come la 
compressione dei dati sia stata una grande necessità; se effettuiamo lo stesso 
calcolo utilizzando una risoluzione standard moderna (e.g., 1920$\times$1080), 
otteniamo un peso di circa 187MB per ogni secondo contenuto nella sequenza 
video. \\
Sebbene a tutt'oggi le capacità di memorizzazione siano incrementate di fattori
che variano tra le 12'500 e le 200'000 volte\footnote{Sono state confrontate
rispettivamente capacità di 500GB e 8TB con la capacità media di 40MB 
dell'epoca.}, la velocità di trasmissione dei dati non è aumentata di pari 
passo; la compressione dei dati rimane comunque anche un'ottima risorsa per
poter immagazzinare grandi quantità di dati con la possibilità di preservare
la qualità originale di questi ultimi.
%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Compressione \emph{lossless}}

Gli algoritmi di compressione senza perdita (\emph{lossless}) solitamente 
sfruttano le ridondanze per rappresentare i dati nel modo più sintetico 
possibile senza andare ad intaccare il messaggio originale: senza cioè alcuna 
perdita d'informazione.\\

Il limite di questa classe di algoritmi è definito dal primo teorema di 
Shannon, che definisce il significato operativo di entropia e vincola la 
massima compressione possibile.
Eccetto alcuni casi particolari è estremamente dispendioso in termini 
computazionali avvicinarsi esattamente al limite teorico della compressione.\\

Queste tecniche sono necessarie laddove non è ammessa la corruzione del dato 
originale: compressione di documenti e programmi ma anche di audio e video ad 
alta qualità, per applicazioni professionali o di estrazione d'informazione da 
parte di un calcolatore.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Compressione \emph{lossy}}

Se l'applicazione non richiede una ricostruzione esatta del messaggio originale 
è possibile utilizzare più efficienti algoritmi di compressione con perdita 
(\emph{lossy}).
Questa classe di algoritmi ha solitamente come fruitore del risultato l'uomo. 
In questo caso vengono sfruttate le conoscenze che si hanno dell'apparato 
audio-visivo al fine di rendere impercettibile la perdita d'informazione, in 
aggiunta a tutte le tecniche di compressione \emph{lossless}.
La resa in termini di dimensioni del compresso è molto superiore rispetto alla 
variante senza perdita. Nel caso di audio ed immagini la dimensione del dato di 
uscita rispetto all'originale passa in media dal $50\%$ della variante 
\emph{lossless} al $10\%$ se viene utilizzata una compressione \emph{lossy}.

%-------------------------------------------------------------------------------
%	SECTION 2
%-------------------------------------------------------------------------------

\section{Compressione di immagini}

% \subsection{Il sistema visivo umano}

%-----------------------------------
%       SUBSECTION 1
%-----------------------------------

\subsection{Utilizzo delle trasformate}

I metodi più diffusi per quanto riguarda la compressione di immagini prevedono 
l'utilizzo di trasformate con lo scopo di rendere l'immagine più adatta ad 
essere codificata.
Essa viene prima divisa in blocchi di dimensioni ridotte, $N \cdot N$, 
solitamente quadrate e non superiori a $64 \cdot 64$.\\

Su ogni blocco viene effettuata separatamente una trasformata, il cui obiettivo 
è quello di de-correlare il più possibile il segnale originale. La bontà di una 
trasformata viene solitamente definita in base alle sue capacità di 
de-correlazione e d'implementazione veloce.\\

Tra le trasformate si ricordano:

\begin{itemize}
  
  \item \textbf{\emph{Discrete Fourier transform} (DFT)}\\
    Largamente utilizzata in analisi e filtraggio, ha la proprietà di avere un 
    nucleo separabile (rendendo possibile quindi il calcolo della trasformata 
    isolatamente su righe e colonne). Ha una sua implementazione veloce, 
    \emph{Fast Fourier transform} (FFT), che rende il suo utilizzo molto 
    appetibile.
    
  \item \textbf{\emph{Karhunen–Loève transform} (KLT)}\\
    Fornisce il miglior compattamento d'energia rispetto alle altre 
    trasformazioni possibili. Purtroppo la mancanza di un algoritmo veloce e la 
    necessità di uno studio della covarianza dell'immagine da comprimere per 
    generare le funzioni base la rendono scarsamente utilizzata.
    
  \item \textbf{\emph{Discrete Cosine transform} (DCT)}\\
    Molto simile alla DFT, utilizza come funzioni base solo coseni.
    Permette di ottenere una de-correlazione molto simile a quella ottenibile 
    trasformando con KLT. La presenza di un algoritmo veloce la rende tutt'ora 
    la trasformata più utilizzata per la compressione d'immagini.
   
  \item \textbf{\emph{Walsh-Hadamard transform} (WHT)}\\
    La peggiore in termini di compattamento d'energia, ha la proprietà di poter 
    essere eseguita con sole somme e sottrazioni ed ha una sua versione 
    \emph{fast}. Il bassissimo costo computazionale l'ha resa largamente 
    utilizzata.
    
\end{itemize}

%-----------------------------------
%       SUBSECTION 2
%-----------------------------------

\subsection{Principali algoritmi}

\paragraph{\textit{Run-length encoding} \\}
  La compressione RLE è una forma molto semplice di codifica d'immagine 
  \emph{lossless}. Applicata solitamente a immagini in bianco e nero, sfrutta 
  il fatto che pixel bianchi e neri si presentano in sequenza. Invece di 
  trasmettere singolarmente i bit dell'immagine vengono trasmessi le lunghezze 
  (\emph{length}) dei tratti (\emph{run}) aventi lo stesso colore.

\paragraph{\textit{Differential Pulse-Code Modulation lossless} \\}
  La codifica DPCM sfrutta la correlazione tra i \emph{pixel} adiacenti di 
  un'immagine: si prova a predire il valore di un pixel a partire da quelli 
  vicini già conosciuti. Questo tipo di codifica permette di avvicinarsi molto 
  al limite massimo di compressione \emph{lossless}.\\
  Il principio di funzionamento è molto semplice: si genera l'immagine predetta 
  e se ne fa la differenza con l'originale, ricavando quindi la funzione 
  ``errore''. Essa sarà poi codificata: il vantaggio rispetto alla semplice 
  codifica dei pixel sta nel fatto che suddetta funzione è molto meno correlata 
  dell'immagine dell'originale.

\paragraph{\textit{Joint Photographic Experts Group} \\}
  JPEG è un metodo molto utilizzato per la compressione \emph{lossy} di 
  immagini digitali, in particolar modo per quelle fotografiche. La codifica 
  JPEG sfrutta alcune peculiarità del sistema visivo umano per poter comprimere 
  il più possibile un'immagine senza che il fruitore (l'uomo) possa accorgersi 
  della differenza dall'orginale. Non esiste una sola versione dell'algoritmo: 
  JPEG/Exif è il più comune nelle immagini generate da fotocamere digitali 
  mentre JPEG/JFIF è utilizzato maggiormente per salvare e trasmettere immagini 
  su Internet, questo per citare solo le due versioni più utilizzate.

%TODO chroma subsampling

%-------------------------------------------------------------------------------
%       SECTION 3
%-------------------------------------------------------------------------------

\section{Compressione video}

La compressione di dati assume un ruolo fondamentale nella trasmissione o 
memorizzazione di sequenze video. Citando l'introduzione: è assolutamente 
impensabile non comprimere uno \emph{stream} a 1080p, che occupa circa 187 MB 
per secondo di video. Vediamo ora i principi base che governano tutti gli 
algoritmi di compressione d'immagini in sequenza.

%-----------------------------------
%       SUBSECTION 1
%-----------------------------------

\subsection{Ridondanza spaziale}

Sfrutta il fatto che ogni immagine (\emph{frame}) del video, presa 
singolarmente, può essere compressa. Valgono infatti tutte le teorie della 
compressione di immagini trattate nel capitolo precedente.
La sola ridondanza spaziale non è però sufficiente per garantire una buona 
dimensione del video compresso, a meno ovviamente di non rinunciare a parecchia 
qualità.

%-----------------------------------
%       SUBSECTION 2
%-----------------------------------

\subsection{Ridondanza temporale}

La vera differenza tra un algoritmo di compressione d'immagine ed uno video sta 
nello sfruttare la ridondanza temporale dei \emph{frame}. Essendo il video 
infatti nient'altro che una serie di fotografie prese ad intervalli costanti e 
molto fitti (il minimo numero possibile affinché l'occhio abbia la percezione 
di movimento è $24$ immagini al secondo), si sfrutta il fatto che tra due 
immagini ``vicine'' temporalmente vi sia una forte correlazione. Solitamente 
infatti, a meno di situazioni particolari, in due \emph{frame} adiacenti vi 
sono gli stessi oggetti spostati di poco dalla loro posizione originale.\\

Questa intuizione permette di andare molto oltre il classico $10\%$ della mera 
compressione basata su ridondanza spaziale arrivando ad addirittura allo 
$0.39\%$\footnote{Utilizzando un video di risoluzione 4k ($3996 \cdot 2160$ 
pixel), H.265 genera un \emph{bit-rate} di circa $3$ MB/s contro i circa $777$ 
della versione non compressa} del file video originale. 

%-----------------------------------
%       SUBSECTION 3
%-----------------------------------

\subsection{Principali algoritmi}

// MPEG-1
// MPEG-2
